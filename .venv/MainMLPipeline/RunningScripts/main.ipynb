{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d84187a6",
   "metadata": {},
   "source": [
    "# Iris dataset (scikit-learn) — quick analysis\n",
    "This file is a **percent-format notebook** (`# %%` cells). You can run it in VS Code / Cursor as a notebook.\n",
    "\n",
    "The analysis is structured as:\n",
    "- Import and dependency checks\n",
    "- Loading the Iris dataset into a typed `DataFrame`\n",
    "- Quick exploratory data analysis (EDA)\n",
    "- Train/test split\n",
    "- Model comparison with cross-validation\n",
    "- Final evaluation of the best model...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a24179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc3be8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    precision_recall_curve,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ HyperParamaters ------------------\n",
    "\n",
    "MultyLablelMinPredictScoreForEval = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ Utilities ------------------\n",
    "\n",
    "HEB_NIQQUD = re.compile(r'[\\u0591-\\u05C7]')  # Hebrew diacritics\n",
    "SPACE_NORMALIZE = re.compile(r'\\s+')\n",
    "\n",
    "SENSITIVE_TERMS = [\n",
    "    \"חיילת\",\"חילת\",\"חייל\",\"חיל\", \"מילואים\", \"מילואימני\",\"מילואימניק\", \"אזאקה\",\"אזעקה\", \"נפילה\",\"מטען\",\"פצצה\",\"פיצוץ\", \"קסאם\", \"אירוע\", \"טיל\",\n",
    "    \"מלחמה\", \"צבע אדום\", \"לחימה\", \"קרב\",\"פינוי\", \"חטוף\",\"חטיפה\"\n",
    "]\n",
    "\n",
    "def normalize_hebrew(text: str) -> str:\n",
    "    \"\"\"Normalize Hebrew text: drop niqqud, unify quotes, collapse spaces.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text\n",
    "    t = HEB_NIQQUD.sub(\"\", t)  # remove niqqud/cantillation\n",
    "    t = t.replace(\"”\", '\"').replace(\"“\", '\"').replace(\"׳\", \"'\").replace(\"״\", '\"')  # unify punctuation\n",
    "    t = SPACE_NORMALIZE.sub(\" \", t).strip()  # de-duplicate whitespace\n",
    "    return t\n",
    "\n",
    "def mask_sensitive(text: str, mask_token: str = \"[MASK]\") -> str:\n",
    "    \"\"\"Mask context-sensitive terms with a placeholder token for robustness checks.\"\"\"\n",
    "    t = text\n",
    "    for w in SENSITIVE_TERMS:\n",
    "        # rf-string: raw regex with formatted escaped word; \\b ensures full-word match\n",
    "        t = re.sub(rf\"\\b{re.escape(w)}\\b\", mask_token, t)\n",
    "    return t\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "    \"\"\"Coerce a DataFrame column to pandas datetime (invalid → NaT).\"\"\"\n",
    "    return pd.to_datetime(df[col], errors=\"coerce\", utc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b41564",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ae02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ Data handling ------------------\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a CSV or Parquet file into a DataFrame.\"\"\"\n",
    "    ext = Path(path).suffix.lower()\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    elif ext in [\".parquet\"]:\n",
    "        df = pd.read_parquet(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "    return df\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Validate schema and normalize text/date; ensure soldier_flag is boolean.\"\"\"\n",
    "    need_cols = [\"id\", \"text\", \"diagnosis\", \"date\", \"soldier_flag\"]\n",
    "    missing = [c for c in need_cols if c not in df.columns]\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    df = df.copy()  # avoid mutating caller's DataFrame\n",
    "\n",
    "    # Text normalization (PII scrubbing assumed upstream)\n",
    "    df[\"text\"] = df[\"text\"].map(normalize_hebrew)\n",
    "\n",
    "    # Date coercion and cleanup of invalid dates\n",
    "    df[\"date\"] = ensure_datetime(df, \"date\")\n",
    "    if df[\"date\"].isna().any():\n",
    "        warnings.warn(\"Some dates could not be parsed and will be dropped.\")\n",
    "        df = df.dropna(subset=[\"date\"])\n",
    "\n",
    "    # Ensure boolean soldier flag (handles 0/1, strings → bool)\n",
    "    if df[\"soldier_flag\"].dtype != bool:\n",
    "        df[\"soldier_flag\"] = df[\"soldier_flag\"].astype(int).astype(bool)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b90df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Split logic ------------------\n",
    "\n",
    "CUTOFF = pd.Timestamp(\"2023-10-07\")\n",
    "\n",
    "def temporal_split(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    pre = df[df[\"date\"] < CUTOFF].copy()\n",
    "    post = df[df[\"date\"] >= CUTOFF].copy()\n",
    "    if len(pre) == 0 or len(post) == 0:\n",
    "        warnings.warn(\"One of the temporal splits is empty.\")\n",
    "    return pre, post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443ea75",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ab63b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ------------------ Labels ------------------\n",
    "\n",
    "def prepare_labels(y: pd.Series, multilabel: bool) -> Tuple[np.ndarray, Any, List[str]]:\n",
    "    \"\"\"\n",
    "    Input: y (Series) of labels; multilabel flag controlling encoding.\n",
    "    Output: (Y, encoder_or_map, classes) where Y is indices or a multilabel binary matrix.\n",
    "    Logic: use MultiLabelBinarizer for multilabel else build class->index map and transform.\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        y_list = y.fillna(\"\").map(lambda s: [t for t in re.split(r\"[|,;/]\", s) if t]).tolist()\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        Y = mlb.fit_transform(y_list)\n",
    "        classes = list(mlb.classes_)\n",
    "        return Y, mlb, classes\n",
    "    else:\n",
    "        classes = sorted(y.dropna().unique().tolist())\n",
    "        class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        y_idx = y.map(class_to_idx).to_numpy()\n",
    "        return y_idx, class_to_idx, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2f742",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86dcbc4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ClinicalFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to create clinical group scores.\n",
    "    Acts like 'Area' calculation by summing occurrences of related terms.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_map: dict):\n",
    "        self.feature_map = feature_map\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "            if not isinstance(X, pd.Series):\n",
    "                if isinstance(X, pd.DataFrame):\n",
    "                    X = X.iloc[:, 0]\n",
    "                else:\n",
    "                    X = pd.Series(np.array(X).ravel())\n",
    "\n",
    "            features = pd.DataFrame(index=range(len(X)))\n",
    "\n",
    "            for group_name, terms in self.feature_map.items():\n",
    "                pattern = '|'.join([rf\"{re.escape(t)}\" for t in terms])\n",
    "                # Ensure numeric count\n",
    "                counts = pd.to_numeric(X.str.count(pattern), errors='coerce').fillna(0)\n",
    "                features[f'{group_name}_score'] = counts\n",
    "\n",
    "            if 'mania' in self.feature_map and 'insomnia' in self.feature_map:\n",
    "                features['mania_x_insomnia'] = (\n",
    "                    features['mania_score'] * features['insomnia_score']\n",
    "                ).fillna(0)\n",
    "\n",
    "            # Convert to float and replace any potential NaN/Inf with 0\n",
    "            output = features.values.astype(np.float64)\n",
    "            return np.nan_to_num(output, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"\n",
    "        Returns the names of the features produced by this transformer.\n",
    "        This prevents AttributeError when called within a Pipeline or FeatureUnion.\n",
    "        \"\"\"\n",
    "        feature_names = [f'{group_name}_score' for group_name in self.feature_map.keys()]\n",
    "\n",
    "        if 'mania' in self.feature_map and 'insomnia' in self.feature_map:\n",
    "            feature_names.append('mania_x_insomnia')\n",
    "\n",
    "        return np.array(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb89d22f",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b683fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ Vectorizers ------------------\n",
    "\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Input: initialized with 'key' (str) of column to select.\n",
    "    Output: at transform, returns the selected column values.\n",
    "    Logic: passthrough transformer to bridge DataFrame to vectorizers.\n",
    "    \"\"\"\n",
    "    def __init__(self, key: str):\n",
    "        \"\"\"\n",
    "        Input: key (str) column name to select.\n",
    "        Output: ColumnSelector instance with stored key.\n",
    "        Logic: save the column name for later transform.\n",
    "        \"\"\"\n",
    "        self.key = key\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Input: X (ignored), y (ignored).\n",
    "        Output: self unchanged.\n",
    "        Logic: stateless transformer; nothing to fit.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Input: X (DataFrame-like) containing the configured column.\n",
    "        Output: numpy array / Series of the selected column.\n",
    "        Logic: return X[self.key].values to feed downstream steps.\n",
    "        \"\"\"\n",
    "        return X[self.key].values\n",
    "\n",
    "def build_vectorizer(max_features_word=50000,\n",
    "                     ngram_word=(1,2),\n",
    "                     use_char=True,\n",
    "                     max_features_char=30000,\n",
    "                     ngram_char=(3,5)):\n",
    "\n",
    "    # 1. Word TF-IDF\n",
    "    word = (\"word\", TfidfVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=ngram_word,\n",
    "        max_features=max_features_word,\n",
    "        min_df=2\n",
    "    ))\n",
    "\n",
    "    # 2. Character TF-IDF (Optional)\n",
    "    transformer_list = [word]\n",
    "    if use_char:\n",
    "        char = (\"char\", TfidfVectorizer(\n",
    "            analyzer=\"char\",\n",
    "            ngram_range=ngram_char,\n",
    "            max_features=max_features_char,\n",
    "            min_df=2\n",
    "        ))\n",
    "        transformer_list.append(char_tfidf)\n",
    "\n",
    "    # 3. Clinical Group Scores with Scaling\n",
    "    # Using MaxAbsScaler ensures scores are scaled to [0, 1] to match TF-IDF\n",
    "    clinical_groups = {\n",
    "        \"psychosis\": [\"דלוזיה\", \"הלוצינציה\", \"פרנואיד\"],\n",
    "        \"depression\": [\"אנהדוניה\", \"ייאוש\", \"ירוד\"],\n",
    "        \"anxiety\": [\"דרוך\", \"מתוח\", \"דופק\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    clinical_pipe = Pipeline([\n",
    "        (\"extractor\", ClinicalFeatureExtractor(clinical_groups)),\n",
    "        (\"scaler\", MaxAbsScaler())\n",
    "    ])\n",
    "\n",
    "    transformer_list.append((\"clinical\", clinical_pipe))\n",
    "\n",
    "    return FeatureUnion(transformer_list)\n",
    "\n",
    "def build_vectorizer_old(max_features_word=50000,\n",
    "                     ngram_word=(1,2),\n",
    "                     use_char=True,\n",
    "                     max_features_char=30000,\n",
    "                     ngram_char=(3,5)):\n",
    "    \"\"\"\n",
    "    Input: word/char TF-IDF settings: features, n-grams, toggles.\n",
    "    Output: FeatureUnion combining word-level TF-IDF and optional char-level TF-IDF.\n",
    "    Logic: instantiate TfidfVectorizer(s) and union them for richer feature space.\n",
    "    \"\"\"\n",
    "    word = (\"word\", TfidfVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=ngram_word,\n",
    "        max_features=max_features_word,\n",
    "        min_df=2\n",
    "    ))\n",
    "    if use_char:\n",
    "        char = (\"char\", TfidfVectorizer(\n",
    "            analyzer=\"char\",\n",
    "            ngram_range=ngram_char,\n",
    "            max_features=max_features_char,\n",
    "            min_df=2\n",
    "        ))\n",
    "        vec = FeatureUnion([word, char])\n",
    "    else:\n",
    "        vec = FeatureUnion([word])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4978da",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Model ------------------\n",
    "def build_model(\n",
    "    base_model: str = \"logreg\",\n",
    "    multilabel: bool = False,\n",
    "    calibrate: bool = True,\n",
    "    class_weight_balanced: bool = True,\n",
    "    random_state: int = 42,\n",
    "    calib_method: str = \"sigmoid\",\n",
    "    calib_cv: int = 5,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Build an estimator for single-label or multilabel.\n",
    "    If multilabel=True and calibrate=True, performs per-label calibration by:\n",
    "      OneVsRestClassifier(CalibratedClassifierCV(base_estimator))\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Choose base estimator\n",
    "    if base_model == \"logreg\":\n",
    "        base = LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            solver=\"saga\",\n",
    "            class_weight=\"balanced\" if class_weight_balanced else None,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    elif base_model == \"linearsvc\":\n",
    "        base = LinearSVC()\n",
    "    else:\n",
    "        raise ValueError(\"base_model must be 'logreg' or 'linearsvc'\")\n",
    "\n",
    "    # 2) Multilabel: per-label model via OneVsRest\n",
    "    if multilabel:\n",
    "        if calibrate:\n",
    "            # Per-label calibration: each OVR binary task gets its own calibrated classifier\n",
    "            base = CalibratedClassifierCV(\n",
    "                estimator=base,\n",
    "                method=calib_method,\n",
    "                cv=calib_cv,\n",
    "            )\n",
    "        clf = OneVsRestClassifier(base, n_jobs=None)\n",
    "        return clf\n",
    "\n",
    "    # 3) Single-label: optionally calibrate the multiclass classifier\n",
    "    clf = base\n",
    "    if calibrate:\n",
    "        clf = CalibratedClassifierCV(\n",
    "            estimator=clf,\n",
    "            method=calib_method,\n",
    "            cv=calib_cv,\n",
    "        )\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340bed4f",
   "metadata": {},
   "source": [
    "### Feature distributions by species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3259718b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ------------------ Training with CV ------------------\n",
    "\n",
    "def small_grid(base_model: str):\n",
    "    \"\"\"\n",
    "    Input: base_model (str) specifying estimator family.\n",
    "    Output: dict hyperparameter grid for regularization C.\n",
    "    Logic: return a compact, robust grid to minimize tuning footprint.\n",
    "    \"\"\"\n",
    "    return {\"C\": [ 0.003,0.001, 0.003, 0.01, 0.03, 0.1]}\n",
    "\n",
    "def stratify_labels(y_array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Input: y_array (ndarray) labels or multilabel indicator matrix.\n",
    "    Output: 1D stratification labels for CV.\n",
    "    Logic: return labels for single-label; for multilabel use row-wise label count.\n",
    "    \"\"\"\n",
    "    if y_array.ndim == 1:\n",
    "        return y_array\n",
    "    else:\n",
    "        return y_array.sum(axis=1)\n",
    "\n",
    "def train_with_cv(X_text: pd.Series,\n",
    "                  y_array: np.ndarray,\n",
    "                  vectorizer: FeatureUnion,\n",
    "                  model: Any,\n",
    "                  classes: List[str],\n",
    "                  base_model: str,\n",
    "                  multilabel: bool,\n",
    "                  n_splits: int = 5) -> Tuple[Any, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Input: texts, encoded labels, vectorizer, estimator, metadata flags, CV splits.\n",
    "    Output: (best_pipeline, info_dict) containing best params and macro-F1.\n",
    "    Logic: build Pipeline, define grids for vectorizer and C, run StratifiedKFold GridSearchCV, refit best.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"select\", ColumnSelector(\"text\")),\n",
    "        (\"vec\", vectorizer),\n",
    "        (\"clf\", model),\n",
    "    ])\n",
    "\n",
    "    #param_grid = {\n",
    "    #    \"vec__transformer_list__word__1__ngram_range\": [(1,1), (1,2)],\n",
    "    #    \"vec__transformer_list__word__1__max_features\": [250,500, 1000,2500,7500],\n",
    "    #}\n",
    "\n",
    "    param_grid = {\n",
    "        \"vec__word__ngram_range\": [(1,1), (1,2)],\n",
    "        \"vec__word__max_features\": [75,100,125,150,200,225, 300, 500, 750],\n",
    "    }\n",
    "\n",
    "    if args.use_char:\n",
    "        param_grid.update({\n",
    "            \"vec__char__ngram_range\": [(3,5)],          # או [(3,5), (3,6)]\n",
    "            \"vec__char__max_features\": [2000, 5000],    # טווח סביר להתחלה\n",
    "        })\n",
    "\n",
    "    C_vals = small_grid(base_model)[\"C\"]\n",
    "    #for key in [\"clf__base_estimator__C\", \"clf__estimator__C\", \"clf__C\"]:\n",
    "    #        param_grid[key] = C_vals\n",
    "    #param_grid[\"clf__estimator__C\"] = C_vals\n",
    "\n",
    "    # Set C at the correct depth depending on wrapper(s)\n",
    "    if hasattr(model, \"estimator\") and hasattr(getattr(model, \"estimator\"), \"estimator\"):\n",
    "        # e.g. CalibratedClassifierCV(estimator=OneVsRestClassifier(estimator=LogReg))\n",
    "        param_grid[\"clf__estimator__estimator__C\"] = C_vals\n",
    "    elif hasattr(model, \"estimator\"):\n",
    "        # e.g. OneVsRestClassifier(estimator=LogReg)\n",
    "        param_grid[\"clf__estimator__C\"] = C_vals\n",
    "    else:\n",
    "        # e.g. plain LogisticRegression\n",
    "        param_grid[\"clf__C\"] = C_vals\n",
    "\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    search = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"f1_macro\",\n",
    "        cv=cv.split(X_text, stratify_labels(y_array)),\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        refit=True,\n",
    "    )\n",
    "    search.fit(pd.DataFrame({\"text\": X_text}), y_array)\n",
    "    best = search.best_estimator_\n",
    "    info = {\n",
    "        \"best_params\": search.best_params_,\n",
    "        \"best_score_macro_f1\": search.best_score_,\n",
    "    }\n",
    "    return best, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a19e6",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Evaluation ------------------\n",
    "\n",
    "def eval_probs(y_true, y_prob, classes, multilabel: bool) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Input: y_true (array/matrix), y_prob (array), classes list, multilabel flag.\n",
    "    Output: metrics dict (macro/micro F1, per-class AUPR, PTSD metrics, Brier mean).\n",
    "    Logic: derive predictions (argmax/threshold), compute aggregate F1/AUPR, PTSD slice, and Brier score.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    if not multilabel:\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        out[\"f1_macro\"] = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        out[\"f1_micro\"] = f1_score(y_true, y_pred, average=\"micro\")\n",
    "        if \"PTSD\" in classes:\n",
    "            idx = classes.index(\"PTSD\")\n",
    "            out[\"f1_ptsd\"] = f1_score((y_true==idx).astype(int), (y_pred==idx).astype(int))\n",
    "            out[\"aupr_ptsd\"] = average_precision_score((y_true==idx).astype(int), y_prob[:, idx])\n",
    "        aupr = {}\n",
    "        for i, c in enumerate(classes):\n",
    "            aupr[c] = average_precision_score((y_true==i).astype(int), y_prob[:, i])\n",
    "        out[\"aupr_per_class\"] = aupr\n",
    "        briers = [brier_score_loss((y_true==i).astype(int), y_prob[:, i]) for i in range(len(classes))]\n",
    "        out[\"brier_ovr_mean\"] = float(np.mean(briers))\n",
    "    else:\n",
    "        #y_pred_bin = (y_prob >= 0.5).astype(int)\n",
    "        y_pred_bin = (y_prob >= MultyLablelMinPredictScoreForEval).astype(int)\n",
    "\n",
    "        out[\"f1_macro\"] = f1_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n",
    "        out[\"f1_micro\"] = f1_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n",
    "        if \"PTSD\" in classes:\n",
    "            idx = classes.index(\"PTSD\")\n",
    "            out[\"f1_ptsd\"] = f1_score(y_true[:, idx], y_pred_bin[:, idx], zero_division=0)\n",
    "            out[\"aupr_ptsd\"] = average_precision_score(y_true[:, idx], y_prob[:, idx])\n",
    "        aupr = {}\n",
    "        for i, c in enumerate(classes):\n",
    "            try:\n",
    "                aupr[c] = average_precision_score(y_true[:, i], y_prob[:, i])\n",
    "            except Exception:\n",
    "                aupr[c] = float(\"nan\")\n",
    "        out[\"aupr_per_class\"] = aupr\n",
    "        bs = []\n",
    "        for i in range(len(classes)):\n",
    "            try:\n",
    "                bs.append(brier_score_loss(y_true[:, i], y_prob[:, i]))\n",
    "            except Exception:\n",
    "                continue\n",
    "        out[\"brier_ovr_mean\"] = float(np.mean(bs)) if bs else float(\"nan\")\n",
    "    return out\n",
    "\n",
    "def predict_proba_safe(model, X_df):\n",
    "    \"\"\"\n",
    "    Input: fitted classifier (possibly calibrated/OVR) and vectorized samples.\n",
    "    Output: probability array for each class/label.\n",
    "    Logic: use predict_proba when available; otherwise map decision_function via logistic transform.\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X_df)\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        dec = model.decision_function(X_df)\n",
    "        if isinstance(dec, list):\n",
    "            dec = np.vstack([d for d in dec]).T\n",
    "        return 1 / (1 + np.exp(-dec))\n",
    "    raise ValueError(\"Model lacks probability outputs. Wrap with calibration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e0fd4",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23758e5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ------------------ Fairness slices ------------------\n",
    "\n",
    "def eval_slices(df: pd.DataFrame, y_true, y_prob, classes, multilabel: bool, slice_cols=(\"soldier_flag\",\"gender\",\"age_group\")) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Input: post DataFrame, true labels, probabilities, class list, multilabel flag, slice column names.\n",
    "    Output: dict of metrics per slice value including false_ptsd_rate.\n",
    "    Logic: create safe 'gender'/'age_group' columns, groupby each slice, run eval_probs, compute false PTSD rate per slice.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    work = df.copy()\n",
    "    if \"gender\" not in work.columns:\n",
    "        work[\"gender\"] = \"NA\"\n",
    "    if \"age\" in work.columns and \"age_group\" not in work.columns:\n",
    "        bins = [0, 25, 40, 60, 200]\n",
    "        labels = [\"<=25\", \"26-40\", \"41-60\", \"60+\"]\n",
    "        work[\"age_group\"] = pd.cut(work[\"age\"], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "    elif \"age_group\" not in work.columns:\n",
    "        work[\"age_group\"] = \"NA\"\n",
    "    for col in slice_cols:\n",
    "        if col not in work.columns:\n",
    "            continue\n",
    "        for val, idx in work.groupby(col).groups.items():\n",
    "            y_t = y_true[idx] if isinstance(y_true, np.ndarray) else y_true.iloc[list(idx)]\n",
    "            y_p = y_prob[idx]\n",
    "            try:\n",
    "                metrics = eval_probs(y_t, y_p, classes, multilabel)\n",
    "                if \"PTSD\" in classes:\n",
    "                    if multilabel:\n",
    "                        ptsd_idx = classes.index(\"PTSD\")\n",
    "                        #fp = np.sum((y_p[:,ptsd_idx] >= 0.5) & (y_t[:,ptsd_idx] == 0))\n",
    "                        fp = np.sum((y_p[:,ptsd_idx] >= MultyLablelMinPredictScoreForEval) & (y_t[:,ptsd_idx] == 0))\n",
    "\n",
    "                        n_neg = np.sum(y_t[:,ptsd_idx] == 0)\n",
    "                        false_rate = float(fp / max(n_neg, 1))\n",
    "                    else:\n",
    "                        ptsd_idx = classes.index(\"PTSD\")\n",
    "                        y_pred = np.argmax(y_p, axis=1)\n",
    "                        fp = np.sum((y_pred == ptsd_idx) & (y_t != ptsd_idx))\n",
    "                        n_neg = np.sum(y_t != ptsd_idx)\n",
    "                        false_rate = float(fp / max(n_neg, 1))\n",
    "                    metrics[\"false_ptsd_rate\"] = false_rate\n",
    "                out[f\"{col}={val}\"] = metrics\n",
    "            except Exception as e:\n",
    "                out[f\"{col}={val}\"] = {\"error\": str(e)}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29161d67",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bdcdc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ------------------ Fairness slices ------------------\n",
    "\n",
    "def eval_slices(\n",
    "    df: pd.DataFrame,\n",
    "    y_true,\n",
    "    y_prob,\n",
    "    classes,\n",
    "    multilabel: bool,\n",
    "    slice_cols=(\"soldier_flag\", \"gender\", \"age_group\"),\n",
    "    do_plots: bool = True,\n",
    "    plots_dir: str | None = None,\n",
    "    plots_prefix: str = \"pr\",\n",
    "    max_groups_per_col: int | None = None,\n",
    "    figsize=(8, 6),\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Input: DataFrame, true labels, probabilities, class list, multilabel flag.\n",
    "    Output: dict of metrics per slice value including false_ptsd_rate.\n",
    "    Optional: create PR plots per group via plot_aupr_per_class (show and or save).\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {}\n",
    "    work = df.copy()\n",
    "\n",
    "    # Ensure slice columns exist\n",
    "    if \"gender\" not in work.columns:\n",
    "        work[\"gender\"] = \"NA\"\n",
    "\n",
    "    if \"age\" in work.columns and \"age_group\" not in work.columns:\n",
    "        bins = [0, 25, 40, 60, 200]\n",
    "        labels = [\"<=25\", \"26-40\", \"41-60\", \"60+\"]\n",
    "        work[\"age_group\"] = pd.cut(\n",
    "            work[\"age\"], bins=bins, labels=labels, right=True, include_lowest=True\n",
    "        )\n",
    "    elif \"age_group\" not in work.columns:\n",
    "        work[\"age_group\"] = \"NA\"\n",
    "\n",
    "    # Prepare plots dir (if saving requested)\n",
    "    if plots_dir is not None:\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "    def _safe_filename(s: str) -> str:\n",
    "        s = str(s)\n",
    "        s = s.strip()\n",
    "        s = s.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\" \", \"_\")\n",
    "        s = re.sub(r\"[^0-9A-Za-zא-ת_+=-]\", \"\", s)\n",
    "        return s[:120] if len(s) > 120 else s\n",
    "\n",
    "    for col in slice_cols:\n",
    "        if col not in work.columns:\n",
    "            continue\n",
    "\n",
    "        groups = list(work.groupby(col).groups.items())\n",
    "        if max_groups_per_col is not None:\n",
    "            groups = groups[:max_groups_per_col]\n",
    "\n",
    "        for val, idx in groups:\n",
    "            idx_list = list(idx)\n",
    "\n",
    "            y_t = y_true[idx_list] if isinstance(y_true, np.ndarray) else y_true.iloc[idx_list]\n",
    "            y_p = y_prob[idx_list]\n",
    "\n",
    "            group_key = f\"{col}={val}\"\n",
    "\n",
    "            try:\n",
    "                metrics = eval_probs(y_t, y_p, classes, multilabel)\n",
    "\n",
    "                # False PTSD rate\n",
    "                if \"PTSD\" in classes:\n",
    "                    ptsd_idx = classes.index(\"PTSD\")\n",
    "                    if multilabel:\n",
    "                        fp = np.sum(\n",
    "                            (y_p[:, ptsd_idx] >= MultyLablelMinPredictScoreForEval) &\n",
    "                            (y_t[:, ptsd_idx] == 0)\n",
    "                        )\n",
    "                        n_neg = np.sum(y_t[:, ptsd_idx] == 0)\n",
    "                    else:\n",
    "                        y_pred = np.argmax(y_p, axis=1)\n",
    "                        fp = np.sum((y_pred == ptsd_idx) & (y_t != ptsd_idx))\n",
    "                        n_neg = np.sum(y_t != ptsd_idx)\n",
    "\n",
    "                    metrics[\"false_ptsd_rate\"] = float(fp / max(n_neg, 1))\n",
    "\n",
    "                out[group_key] = metrics\n",
    "\n",
    "                # Plot per group (one figure per group)\n",
    "                if do_plots or plots_dir is not None:\n",
    "                    safe_val = _safe_filename(val)\n",
    "                    safe_col = _safe_filename(col)\n",
    "                    fname = f\"{plots_prefix}_{safe_col}_{safe_val}.png\"\n",
    "\n",
    "                    plot_aupr_per_class(\n",
    "                        Y_true=y_t,\n",
    "                        Y_prob=y_p,\n",
    "                        classes=classes,\n",
    "                        figsize=figsize,\n",
    "                        do_plots=do_plots,\n",
    "                        plots_dir=plots_dir,\n",
    "                        title=f\"Precision Recall | {group_key} | n={len(idx_list)}\",\n",
    "                        filename=fname,\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                out[group_key] = {\"error\": str(e)}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921416be",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Drift: PSI and KL ------------------\n",
    "\n",
    "def psi(expected: np.ndarray, actual: np.ndarray, bins: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Input: expected and actual numeric arrays; number of bins.\n",
    "    Output: float PSI value indicating distribution shift (lower ~ more stable).\n",
    "    Logic: bin both on shared edges, normalize, avoid zeros, apply PSI formula sum((a-e)*ln(a/e)).\n",
    "    \"\"\"\n",
    "    ex, bin_edges = np.histogram(expected, bins=bins)\n",
    "    ac, _ = np.histogram(actual, bins=bin_edges)\n",
    "    ex = ex / max(ex.sum(), 1)\n",
    "    ac = ac / max(ac.sum(), 1)\n",
    "    ex = np.where(ex==0, 1e-6, ex)\n",
    "    ac = np.where(ac==0, 1e-6, ac)\n",
    "    return float(np.sum((ac - ex) * np.log(ac / ex)))\n",
    "\n",
    "def kl_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Input: arrays p and q representing discrete distributions over same support.\n",
    "    Output: float KL divergence D(P||Q) (lower ~ more similar).\n",
    "    Logic: normalize to sum 1, clip zeros to small constants, compute sum p*log(p/q).\n",
    "    \"\"\"\n",
    "    p = p / max(p.sum(), 1)\n",
    "    q = q / max(q.sum(), 1)\n",
    "    p = np.where(p==0, 1e-12, p)\n",
    "    q = np.where(q==0, 1e-12, q)\n",
    "    return float(np.sum(p * np.log(p / q)))\n",
    "\n",
    "def drift_top_terms(vec, X_pre_text: pd.Series, X_post_text: pd.Series, top_k: int = 50) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Input: vectorizer-like object, pre/post text Series, and top_k feature count.\n",
    "    Output: dict feature->{pre_mean, post_mean, delta} and aggregate {_PSI, _KL}.\n",
    "    Logic: clone vec and fit on pre, compute mean TF-IDF, select top_k, compare with post, compute PSI/KL over selected slice.\n",
    "    \"\"\"\n",
    "    V = clone(vec)\n",
    "    X_pre = V.fit_transform(X_pre_text)\n",
    "    feature_names = []\n",
    "    for name, trans in V.transformer_list:\n",
    "        if hasattr(trans, \"get_feature_names_out\"):\n",
    "            names = [f\"{name}::{n}\" for n in trans.get_feature_names_out()]\n",
    "            feature_names.extend(names)\n",
    "    pre_mean = np.asarray(X_pre.mean(axis=0)).ravel()\n",
    "    top_idx = np.argsort(pre_mean)[::-1][:top_k]\n",
    "    top_feats = [feature_names[i] for i in top_idx]\n",
    "    X_post = V.transform(X_post_text)\n",
    "    post_mean = np.asarray(X_post.mean(axis=0)).ravel()\n",
    "    res = {}\n",
    "    for i, feat in zip(top_idx, top_feats):\n",
    "        p = pre_mean[i]\n",
    "        q = post_mean[i]\n",
    "        res[feat] = {\"pre_mean\": float(p), \"post_mean\": float(q), \"delta\": float(q - p)}\n",
    "    distr_pre = pre_mean[top_idx]\n",
    "    distr_post = post_mean[top_idx]\n",
    "    res[\"_PSI\"] = {\"psi\": psi(distr_pre, distr_post, bins=min(10, len(distr_pre)))}\n",
    "    res[\"_KL\"]  = {\"kl\": kl_divergence(distr_pre, distr_post)}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94f2e7",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176ae9c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ------------------ Explainability ------------------\n",
    "\n",
    "def top_coefficients(model, vec, classes: List[str], k: int = 20) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Input: trained model (possibly wrapped), fitted vectorizer, classes list, top k.\n",
    "    Output: dict mapping class->list of (feature_name, weight) or info if unavailable.\n",
    "    Logic: unwrap calibrated/OVR estimators, access coef_, map to feature names, select top-k per class.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    feature_names = []\n",
    "    for name, trans in vec.transformer_list:\n",
    "        if hasattr(trans, \"get_feature_names_out\"):\n",
    "            names = [f\"{name}::{n}\" for n in trans.get_feature_names_out()]\n",
    "            feature_names.extend(names)\n",
    "    # Attempt to unwrap calibrated models\n",
    "    if isinstance(model, CalibratedClassifierCV):\n",
    "        base = getattr(model, \"base_estimator\", getattr(model, \"estimator\", model))\n",
    "    else:\n",
    "        base = model\n",
    "    if isinstance(base, OneVsRestClassifier):\n",
    "        for i, est in enumerate(base.estimators_):\n",
    "            if hasattr(est, \"coef_\"):\n",
    "                coefs = est.coef_.ravel()\n",
    "                top_pos = np.argsort(coefs)[-k:][::-1]\n",
    "                out[classes[i]] = [(feature_names[j], float(coefs[j])) for j in top_pos]\n",
    "    elif hasattr(base, \"coef_\"):\n",
    "        for i, cls in enumerate(classes):\n",
    "            top_pos = np.argsort(base.coef_[i])[-k:][::-1]\n",
    "            out[cls] = [(feature_names[j], float(base.coef_[i][j])) for j in top_pos]\n",
    "    else:\n",
    "        out[\"info\"] = [\"Coefficients unavailable for this model.\"]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2512a7",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33d8ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ graphics ------------------\n",
    "\n",
    "def plot_aupr_per_class(\n",
    "    Y_true,\n",
    "    Y_prob,\n",
    "    classes,\n",
    "    figsize=(8, 6),\n",
    "    do_plots: bool = True,\n",
    "    plots_dir: str | None = None,\n",
    "    title: str | None = None,\n",
    "    filename: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves and AUPR for each class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_true : np.ndarray\n",
    "        True labels.\n",
    "        Shape (n_samples,) for single-label\n",
    "        or (n_samples, n_classes) for multilabel.\n",
    "\n",
    "    Y_prob : np.ndarray\n",
    "        Predicted probabilities.\n",
    "        Shape (n_samples, n_classes).\n",
    "\n",
    "    classes : list[str]\n",
    "        Class names, order must match Y_prob columns.\n",
    "\n",
    "    do_plots : bool\n",
    "        Whether to display the plot.\n",
    "\n",
    "    plots_dir : str or None\n",
    "        If provided, save plot as PNG to this directory.\n",
    "\n",
    "    title : str or None\n",
    "        Optional plot title.\n",
    "\n",
    "    filename : str or None\n",
    "        Optional filename (without path). If None, auto-generated.\n",
    "    \"\"\"\n",
    "\n",
    "    if not do_plots and plots_dir is None:\n",
    "        return  # nothing to do\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        if Y_true.ndim == 1:\n",
    "            y_true_bin = (Y_true == i).astype(int)\n",
    "        else:\n",
    "            y_true_bin = Y_true[:, i]\n",
    "\n",
    "        y_prob_cls = Y_prob[:, i]\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(\n",
    "            y_true_bin,\n",
    "            y_prob_cls\n",
    "        )\n",
    "        aupr = average_precision_score(\n",
    "            y_true_bin,\n",
    "            y_prob_cls\n",
    "        )\n",
    "\n",
    "        plt.plot(\n",
    "            recall,\n",
    "            precision,\n",
    "            lw=2,\n",
    "            label=f\"{cls} (AUPR={aupr:.3f})\"\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(title if title else \"Precision–Recall Curves per Diagnosis\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ---- Save plot if requested ----\n",
    "    if plots_dir is not None:\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        if filename is None:\n",
    "            filename = \"pr_curves.png\"\n",
    "        plt.savefig(\n",
    "            os.path.join(plots_dir, filename),\n",
    "            dpi=200,\n",
    "            bbox_inches=\"tight\"\n",
    "        )\n",
    "\n",
    "    # ---- Show plot if requested ----\n",
    "    if do_plots:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da44480",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26359e1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# === Step 1: Parse CLI arguments ============================================================\n",
    "p = argparse.ArgumentParser(description=\"Hebrew clinical text classification pipeline.\")\n",
    "\n",
    "p.add_argument(\n",
    "    \"--data\",\n",
    "    default=r\"C:\\Users\\sagil\\GEAH\\MockDataSatatus.csv\",\n",
    "    help=\"Path to CSV or Parquet with id,text,diagnosis,date,soldier_flag\",\n",
    ")\n",
    "\n",
    "p.add_argument(\"--multilabel\", action=\"store_true\", default=False, help=\"Treat diagnosis as multi-label string\")\n",
    "p.add_argument(\"--base_model\", choices=[\"logreg\", \"linearsvc\"], default=\"logreg\")\n",
    "p.add_argument(\"--use_char\", action=\"store_true\", default=False, help=\"Add char n-grams 3-5\")\n",
    "p.add_argument(\"--max_features_word\", type=int, default=5000)\n",
    "p.add_argument(\"--max_features_char\", type=int, default=2500)\n",
    "p.add_argument(\"--ngram_word_max\", type=int, default=2)\n",
    "p.add_argument(\"--no_calibrate\", action=\"store_true\", default=False, help=\"Disable probability calibration\")\n",
    "p.add_argument(\"--output_dir\", default=\"/content/drive/MyDrive/MyPapers/PTSD-MisDiagnosis/Info/output\", help=\"Directory to write JSON artifacts\")\n",
    "p.add_argument(\"--mask_sensitive_test\", action=\"store_true\", default=False, help=\"Run masking robustness test on post split\")\n",
    "\n",
    "if \"ipykernel\" in sys.modules:\n",
    "    args = p.parse_args([])\n",
    "else:\n",
    "    args = p.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01172dc1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# === Step 2: Ensure output directory exists ================================================\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "graphs_dir = os.path.join(args.output_dir, \"graphs\")\n",
    "os.makedirs(graphs_dir, exist_ok=True)\n",
    "\n",
    "# === Step 3: Load & preprocess raw data ====================================================\n",
    "# - Normalizes Hebrew text, coerces date, validates schema, boolean soldier_flag\n",
    "df = load_data(args.data)\n",
    "df = preprocess_df(df)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74f3c7",
   "metadata": {},
   "source": [
    "=== Step 4: Temporal split (train before 2023-10-07; evaluate on/after) ===================\n",
    "pre, post = temporal_split(df)\n",
    "if len(pre) == 0 or len(post) == 0:\n",
    "    # We require both sides to exist to avoid leakage and to test generalization post-cutoff\n",
    "    print(\"ERROR: Pre or Post split empty.\", file=sys.stderr)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ff1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Step 5: Encode labels on pre only =====================================================\n",
    "# - Prevents peeking at post distribution/classes during training\n",
    "Y_pre, enc, classes = prepare_labels(pre[\"diagnosis\"], multilabel=args.multilabel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518385b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 6: Define vectorizer & model =====================================================\n",
    "# - Word n-grams + optional char n-grams; linear model with optional calibration\n",
    "vec = build_vectorizer(\n",
    "    max_features_word=args.max_features_word,\n",
    "    ngram_word=(1, args.ngram_word_max),\n",
    "    use_char=args.use_char,\n",
    "    max_features_char=args.max_features_char,\n",
    "    ngram_char=(3, 5),\n",
    ")\n",
    "model = build_model(\n",
    "    base_model=args.base_model,\n",
    "    multilabel=args.multilabel,\n",
    "    calibrate=not args.no_calibrate,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 7: Cross-validated training on pre ===============================================\n",
    "# - Small grid over vectorizer and C; macro-F1 scoring; refit best pipeline\n",
    "\n",
    "\n",
    "print(\"args.multilabel:\", args.multilabel)\n",
    "print(\"Y_pre shape:\", getattr(Y_pre, \"shape\", None))\n",
    "\n",
    "best, info = train_with_cv(\n",
    "    pre[\"text\"], Y_pre, vec, model, classes, args.base_model, args.multilabel\n",
    ")\n",
    "with open(os.path.join(args.output_dir, \"cv_best.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(info, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187b443",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5422aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 8: Lock feature space for drift/explainability ===================================\n",
    "# - Fit a clean vectorizer on all pre text to get stable feature names for analyses\n",
    "V = build_vectorizer(\n",
    "    max_features_word=args.max_features_word,\n",
    "    ngram_word=(1, args.ngram_word_max),\n",
    "    use_char=args.use_char,\n",
    "    max_features_char=args.max_features_char,\n",
    "    ngram_char=(3, 5),\n",
    ")\n",
    "V.fit(pre[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734b916",
   "metadata": {},
   "source": [
    "    # ### Model comparison: Detailed CV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 9: Prepare post labels in the pre-learned class space ============================\n",
    "# - Single-label: drop unseen labels; Multi-label: fix the class order using pre classes\n",
    "if args.multilabel:\n",
    "    y_post_list = post[\"diagnosis\"].fillna(\"\").map(\n",
    "        lambda s: [t for t in re.split(r\"[|,;/]\", s) if t]\n",
    "    ).tolist()\n",
    "    Y_post = MultiLabelBinarizer(classes=classes).fit(classes).transform(y_post_list)\n",
    "else:\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_post_idx = post[\"diagnosis\"].map(class_to_idx).fillna(-1).astype(int)\n",
    "    keep = y_post_idx >= 0\n",
    "    dropped = (~keep).sum()\n",
    "    if dropped > 0:\n",
    "        warnings.warn(f\"Dropping {dropped} post rows with unseen labels.\")\n",
    "    post = post[keep].copy()\n",
    "    Y_post = y_post_idx[keep].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2e610",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcab9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 10: Evaluate best model on post split ============================================\n",
    "# - Use the fitted steps from the CV best pipeline to avoid leakage\n",
    "vec_step = best.named_steps[\"vec\"]\n",
    "clf_step = best.named_steps[\"clf\"]\n",
    "\n",
    "# Transform post text with the *trained* vectorizer\n",
    "#X_post_vec = vec_step.transform(pd.DataFrame({\"text\": post[\"text\"]}))\n",
    "X_post_text = post[\"text\"].astype(str).tolist()\n",
    "X_post_vec = vec_step.transform(X_post_text)\n",
    "#pdb.set_trace()\n",
    "\n",
    "# Get probabilities (calibrated when enabled; logistic link fallback otherwise)\n",
    "y_prob_post = (\n",
    "    clf_step.predict_proba(X_post_vec)\n",
    "    if hasattr(clf_step, \"predict_proba\")\n",
    "    else predict_proba_safe(clf_step, X_post_vec)\n",
    ")\n",
    "print(type(Y_post), getattr(Y_post, \"shape\", None), len(Y_post))\n",
    "print(type(y_prob_post), getattr(y_prob_post, \"shape\", None), len(y_prob_post))\n",
    "\n",
    "if isinstance(y_prob_post, list):\n",
    "    print(\"list length:\", len(y_prob_post))\n",
    "    print(\"first element type/shape:\", type(y_prob_post[0]), getattr(y_prob_post[0], \"shape\", None))\n",
    "metrics_post = eval_probs(Y_post, y_prob_post, classes, args.multilabel)\n",
    "with open(os.path.join(args.output_dir, \"metrics_post.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_post, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09d06c",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aupr_per_class(\n",
    "    Y_true=Y_post,\n",
    "    Y_prob=y_prob_post,\n",
    "    classes=classes,\n",
    "    plots_dir=graphs_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b511705",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# === Step 11: Fairness slices metrics ======================================================\n",
    "# - soldier_flag / gender / age_group; includes false PTSD rate per slice\n",
    "slices = eval_slices(\n",
    "    post.reset_index(drop=True), Y_post, y_prob_post, classes, args.multilabel,plots_dir=graphs_dir,do_plots = False\n",
    ")\n",
    "with open(os.path.join(args.output_dir, \"slices_post.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(slices, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444fe26",
   "metadata": {},
   "source": [
    "### Feature correlation with target (class separation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 12: Distribution shift (drift) around top pre terms ==============================\n",
    "drift = drift_top_terms(V, pre[\"text\"], post[\"text\"], top_k=50)\n",
    "with open(os.path.join(args.output_dir, \"drift.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(drift, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66371783",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e02241",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# === Step 13: Explainability via coefficients (plain model) ================================\n",
    "# Refit a NON-calibrated model to expose coef_ cleanly; map to feature names\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# 1) Start from the CV-best pipeline params\n",
    "best_params = best.get_params()\n",
    "\n",
    "# 2) Ensure the explainability vectorizer uses the SAME hyperparams as the best pipeline\n",
    "vec_params = {k.replace(\"vec__\", \"\"): v for k, v in best_params.items() if k.startswith(\"vec__\")}\n",
    "if vec_params:\n",
    "    try:\n",
    "        V.set_params(**vec_params)\n",
    "    except Exception:\n",
    "        # Keep V as-is if params are incompatible\n",
    "        pass\n",
    "\n",
    "# 3) Build a NON-calibrated classifier\n",
    "best_clf = best.named_steps[\"clf\"]\n",
    "core_est = getattr(best_clf, \"estimator\", best_clf)\n",
    "\n",
    "# Ensure the core estimator is wrapped in OneVsRest if multilabel is enabled\n",
    "if args.multilabel:\n",
    "    if not isinstance(core_est, OneVsRestClassifier):\n",
    "        plain_clf = OneVsRestClassifier(clone(core_est))\n",
    "    else:\n",
    "        plain_clf = clone(core_est)\n",
    "else:\n",
    "    plain_clf = clone(core_est)\n",
    "\n",
    "# Copy any clf__ params if they exist\n",
    "clf_params = {k.replace(\"clf__\", \"\"): v for k, v in best_params.items() if k.startswith(\"clf__\")}\n",
    "if clf_params:\n",
    "    try:\n",
    "        plain_clf.set_params(**clf_params)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# 4) Fit explainability pipeline on all pre data\n",
    "expl_pipe = Pipeline([\n",
    "    (\"select\", ColumnSelector(\"text\")),\n",
    "    (\"vec\", V),\n",
    "    (\"clf\", plain_clf),\n",
    "])\n",
    "\n",
    "# This will now accept Y_pre as a matrix when multilabel is True\n",
    "expl_pipe.fit(pd.DataFrame({\"text\": pre[\"text\"]}), Y_pre)\n",
    "\n",
    "# 5) Extract top coefficients mapped to feature names\n",
    "# The top_coefficients function in your code already handles OneVsRestClassifier\n",
    "coef_top = top_coefficients(expl_pipe.named_steps[\"clf\"], V, classes, k=20)\n",
    "\n",
    "with open(os.path.join(args.output_dir, \"explain_top_coeffs.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(coef_top, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5914e5",
   "metadata": {},
   "source": [
    "Explain a bit what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25134b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# === Step 14: Optional masking robustness test =============================================\n",
    "# - Replace context-sensitive terms (e.g., חייל/מילואים) with [MASK] and re-evaluate deltas\n",
    "\n",
    "if args.mask_sensitive_test:\n",
    "    post_masked = post.copy()\n",
    "    post_masked[\"text\"] = post_masked[\"text\"].map(mask_sensitive)\n",
    "\n",
    "    X_mask_text = post_masked[\"text\"].astype(str).tolist()\n",
    "    X_mask_vec = vec_step.transform(X_mask_text)\n",
    "\n",
    "    y_prob_mask = (\n",
    "        clf_step.predict_proba(X_mask_vec)\n",
    "        if hasattr(clf_step, \"predict_proba\")\n",
    "        else predict_proba_safe(clf_step, X_mask_vec)\n",
    "    )\n",
    "\n",
    "    metrics_mask = eval_probs(Y_post, y_prob_mask, classes, args.multilabel)\n",
    "\n",
    "    delta = {}\n",
    "    for k in metrics_post:\n",
    "        if isinstance(metrics_post[k], dict):\n",
    "            continue\n",
    "        try:\n",
    "            delta[k] = float(metrics_mask[k]) - float(metrics_post[k])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    out = {\"masked_metrics\": metrics_mask, \"delta_vs_unmasked\": delta}\n",
    "    with open(os.path.join(args.output_dir, \"masking_eval.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
